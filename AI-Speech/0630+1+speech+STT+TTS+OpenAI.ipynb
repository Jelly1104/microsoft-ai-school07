{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53408756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "252ea8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio \n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    def change_audio(audio_path):\n",
    "        return audio_path\n",
    "\n",
    "    with gr.Column():\n",
    "        input_audio=gr.Audio(sources=\"microphone\",type=\"filepath\",label=\"마이크입력\")\n",
    "        output_text=gr.Textbox(label=\"음성 인식 결과\", placeholder=\"여기에 음성 인식 결과가 표시됩니다.\", interactive=False)\n",
    "\n",
    "        input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c295bff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STT (Speech to Text)\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "azure_stt_endpoint=os.getenv(\"azure_stt_endpoint\")\n",
    "azure_stt_key=os.getenv(\"azure_stt_key\")\n",
    "azure_tts_endpoint=os.getenv(\"azure_tts_endpoint\")\n",
    "azure_tts_key=os.getenv(\"azure_tts_key\")\n",
    "\n",
    "\n",
    "def request_stt(audio_path):\n",
    "    \n",
    "    endpoints=azure_stt_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_stt_key\n",
    "        }\n",
    "    \n",
    "    with open(audio_path, 'rb') as audio_files:\n",
    "        audio_data=audio_files.read()\n",
    "\n",
    "    response = requests.post(endpoints,headers=headers,data=audio_data)\n",
    "    #print(response)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    \n",
    "    response_json=response.json()\n",
    "    content=response_json['DisplayText']\n",
    "    return content\n",
    "    \n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "\n",
    "    def change_audio(audio_path):\n",
    "        if audio_path is None:\n",
    "            return None\n",
    "        \n",
    "        content = request_stt(audio_path)\n",
    "        return content\n",
    "\n",
    "\n",
    "    with gr.Column():\n",
    "        input_audio = gr.Audio(sources=\"microphone\", type=\"filepath\",label=\"마이크 입력\")\n",
    "        output_text = gr.Textbox(label=\"음성 인식 결과\", placeholder=\"여기에 음성 인식이 표시됩니다.\", interactive=False)\n",
    "\n",
    "        input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccca08e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STT (Speech to Text)\n",
    "# TTS (Text to Speech)\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "azure_stt_endpoint=os.getenv(\"azure_stt_endpoint\")\n",
    "azure_stt_key=os.getenv(\"azure_stt_key\")\n",
    "azure_tts_endpoint=os.getenv(\"azure_tts_endpoint\")\n",
    "azure_tts_key=os.getenv(\"azure_tts_key\")\n",
    "\n",
    "def request_stt(audio_path):\n",
    "    \n",
    "    endpoints=azure_stt_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_stt_key\n",
    "        }\n",
    "    \n",
    "    with open(audio_path, 'rb') as audio_files:\n",
    "        audio_data=audio_files.read()\n",
    "\n",
    "    response = requests.post(endpoints,headers=headers,data=audio_data)\n",
    "    #print(response)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    \n",
    "    response_json=response.json()\n",
    "    content=response_json['DisplayText']\n",
    "    return content\n",
    "\n",
    "def request_tts(text):\n",
    "    \n",
    "    endpoints=azure_tts_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_tts_key,\n",
    "        \"Content-Type\":\"application/ssml+xml\",\n",
    "        \"X-Microsoft-OutputFormat\":\"audio-16khz-128kbitrate-mono-mp3\"\n",
    "        }\n",
    "    body = f\"\"\"\n",
    "        <speak version='1.0' xml:lang='ko-KR'>\n",
    "            <voice xml:lang='ko-KR' xml:gender='Female' name='ko-KR-SunHiNeural'>\n",
    "                {text}\n",
    "            </voice>\n",
    "        </speak>\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(endpoints, headers=headers, data=body)\n",
    "    #print(response)\n",
    "\n",
    "    if response.status_code !=200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    import datetime\n",
    "    now=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    filename = f\"tts_result{now}.wav\"\n",
    "\n",
    "    with open(filename,\"wb\") as audio_files:\n",
    "        audio_files.write(response.content)\n",
    "\n",
    "        return filename\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "\n",
    "    def change_audio(audio_path):\n",
    "        if audio_path is None:\n",
    "            return None\n",
    "        \n",
    "        content = request_stt(audio_path)\n",
    "        return content\n",
    "        \n",
    "    def click_send_tts(text):\n",
    "        filename=request_tts(text)\n",
    "        return filename\n",
    "\n",
    "    #STT\n",
    "    with gr.Column():\n",
    "        gr.Markdown(\"### STT\")\n",
    "        input_audio = gr.Audio(sources=\"microphone\", type=\"filepath\",label=\"마이크 입력\")\n",
    "        output_text = gr.Textbox(label=\"음성 인식 결과\", placeholder=\"여기에 음성 인식이 표시됩니다.\", interactive=False)\n",
    "\n",
    "        input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "\n",
    "    #TTS\n",
    "    with gr.Column():\n",
    "        gr.Markdown(\"### TTS\")\n",
    "        tts_textbox = gr.Textbox(label=\"텍스트 입력\", placeholder=\"텍스트를 입력하세요\")\n",
    "        send_tts_button = gr.Button(\"음성으로 변환\")\n",
    "        output_tts_audio = gr.Audio(label=\"음성 출력\", type=\"filepath\", interactive=False,autoplay=True)\n",
    "\n",
    "    input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "    send_tts_button.click(click_send_tts, inputs=[tts_textbox],outputs=[output_tts_audio])\n",
    "\n",
    "demo.launch()\n",
    "#click_send_tts(\"안녕하세요. 음성 인식과 음성 합성 기능을 테스트합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b8c01",
   "metadata": {},
   "source": [
    "![최종 구현 화면](./0630+최종구현화면+예시.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a590c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45b2cdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STT (Speech to Text)\n",
    "# TTS (Text to Speech)\n",
    "# Gradio Wrapper (좌:chatbot 우:STT, TTS)\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "azure_stt_endpoint=os.getenv(\"azure_stt_endpoint\")\n",
    "azure_stt_key=os.getenv(\"azure_stt_key\")\n",
    "azure_tts_endpoint=os.getenv(\"azure_tts_endpoint\")\n",
    "azure_tts_key=os.getenv(\"azure_tts_key\")\n",
    "\n",
    "def request_stt(audio_path):\n",
    "    \n",
    "    endpoints=azure_stt_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_stt_key\n",
    "        }\n",
    "    \n",
    "    with open(audio_path, 'rb') as audio_files:\n",
    "        audio_data=audio_files.read()\n",
    "\n",
    "    response = requests.post(endpoints,headers=headers,data=audio_data)\n",
    "    #print(response)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    \n",
    "    response_json=response.json()\n",
    "    content=response_json['DisplayText']\n",
    "    return content\n",
    "\n",
    "def request_tts(text):\n",
    "    \n",
    "    endpoints=azure_tts_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_tts_key,\n",
    "        \"Content-Type\":\"application/ssml+xml\",\n",
    "        \"X-Microsoft-OutputFormat\":\"audio-16khz-128kbitrate-mono-mp3\"\n",
    "        }\n",
    "    body = f\"\"\"\n",
    "        <speak version='1.0' xml:lang='ko-KR'>\n",
    "            <voice xml:lang='ko-KR' xml:gender='Female' name='ko-KR-SunHiNeural'>\n",
    "                {text}\n",
    "            </voice>\n",
    "        </speak>\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(endpoints, headers=headers, data=body)\n",
    "    #print(response)\n",
    "\n",
    "    if response.status_code !=200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    import datetime\n",
    "    now=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    filename = f\"tts_result{now}.wav\"\n",
    "\n",
    "    with open(filename,\"wb\") as audio_files:\n",
    "        audio_files.write(response.content)\n",
    "\n",
    "        return filename\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "\n",
    "    def change_audio(audio_path):\n",
    "        if audio_path is None:\n",
    "            return None\n",
    "        \n",
    "        content = request_stt(audio_path)\n",
    "        return content\n",
    "        \n",
    "    def click_send_tts(text):\n",
    "        filename=request_tts(text)\n",
    "        return filename\n",
    "    \n",
    "    #Wrapper \n",
    "    with gr.Row(): #가로 배치\n",
    "        #좌측 (챗봇)\n",
    "        with gr.Column():\n",
    "            chatbot=gr.Chatbot(type=\"messages\")\n",
    "\n",
    "        #우측 (STT, TTS) \n",
    "        with gr.Column():\n",
    "            #STT\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### STT\")\n",
    "                input_audio = gr.Audio(sources=\"microphone\", type=\"filepath\",label=\"마이크 입력\")\n",
    "                output_text = gr.Textbox(label=\"음성 인식 결과\", placeholder=\"여기에 음성 인식이 표시됩니다.\", interactive=False)\n",
    "\n",
    "                input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "            #TTS\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### TTS\")\n",
    "                tts_textbox = gr.Textbox(label=\"텍스트 입력\", placeholder=\"텍스트를 입력하세요\")\n",
    "                send_tts_button = gr.Button(\"음성으로 변환\")\n",
    "                output_tts_audio = gr.Audio(label=\"음성 출력\", type=\"filepath\", interactive=False,autoplay=True)\n",
    "\n",
    "    input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "    send_tts_button.click(click_send_tts, inputs=[tts_textbox],outputs=[output_tts_audio])\n",
    "\n",
    "demo.launch()\n",
    "#click_send_tts(\"안녕하세요. 음성 인식과 음성 합성 기능을 테스트합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d36a22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}, 'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'annotations': [], 'content': '안녕하세요! \"값\"이라고 하셨는데, 좀 더 구체적으로 어떤 값이 필요하신가요? 아래는 \"값\"이라는 단어에 대한 몇 가지 정보입니다:\\n\\n**1. 국어사전 의미:**  \\n- 어떤 수량이나 수치, 혹은 가격이나 가치 등을 나타내는 명사입니다.  \\n  예:  \\n  - 이 상품의 값이 얼마인가요?  \\n  - 변수의 값을 할당하다.\\n\\n**2. 프로그래밍에서의 \"값\":**  \\n- 변수나 상수에 저장되어 있는 데이터(숫자, 문자열 등)를 의미합니다.  \\n  예:  \\n  ```python\\n  x = 10  # 여기서 10이 x의 값입니다.\\n  ```\\n\\n**3. 수학에서의 \"값\":**  \\n- 식이나 함수의 결과로 나오는 수.  \\n  예:  \\n  - 2 + 3의 값은 5입니다.\\n\\n궁금하신 \"값\"에 대해 좀 더 구체적으로 질문해주시면, 더 알맞은 답변을 드릴 수 있습니다! 어떤 분야나 맥락에 대해 물으시는 건가요?', 'refusal': None, 'role': 'assistant'}}], 'created': 1751407907, 'id': 'chatcmpl-BodUZHiMTjt6Sw8qTe3raBXh5hL0x', 'model': 'gpt-4.1-2025-04-14', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'system_fingerprint': 'fp_07e970ab25', 'usage': {'completion_tokens': 249, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens': 8, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'total_tokens': 257}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from openai import AzureOpenAI \n",
    "\n",
    "# GPT API 요청 함수 \n",
    "endpoint = \"https://fimtrus-ai-project-resource.cognitiveservices.azure.com/openai/deployments/fimtrus-gpt-41/chat/completions?api-version=2025-01-01-preview\"\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer 9VEZmMXEZd4aWzFIEVBKCse1NEa3en2LrD51oEyXbFc41XDbcP2VJQQJ99BFACYeBjFXJ3w3AAAAACOGydeD\"\n",
    "}\n",
    "body = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"값\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_completion_tokens\": 800,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 1,\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"presence_penalty\": 0,\n",
    "    \"model\": \"fimtrus-gpt-41\"\n",
    "}\n",
    "response = requests.post(endpoint, headers=headers, json=body)\n",
    "response_json=response.json()\n",
    "print(response_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51bf3027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (5.35.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (0.115.12)\n",
      "Requirement already satisfied: ffmpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (0.6.0)\n",
      "Requirement already satisfied: gradio-client==1.10.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (1.10.4)\n",
      "Requirement already satisfied: groovy~=0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (0.33.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: packaging in /Users/eunahjeong/Library/Python/3.11/lib/python/site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (11.2.1)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (2.11.7)\n",
      "Requirement already satisfied: pydub in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (0.11.13)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /Users/eunahjeong/Library/Python/3.11/lib/python/site-packages (from gradio) (4.13.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio) (0.34.3)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio-client==1.10.4->gradio) (2025.5.1)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gradio-client==1.10.4->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.28.1->gradio) (1.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/eunahjeong/Library/Python/3.11/lib/python/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/eunahjeong/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/eunahjeong/Library/Python/3.11/lib/python/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd5f1849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STT (Speech to Text)\n",
    "# TTS (Text to Speech)\n",
    "# Gradio Wrapper (좌:chatbot 우:STT, TTS)\n",
    "# Open AI\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "from openai import AzureOpenAI \n",
    "import os\n",
    "import pdb\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "azure_stt_endpoint=os.getenv(\"azure_stt_endpoint\")\n",
    "azure_stt_key=os.getenv(\"azure_stt_key\")\n",
    "azure_tts_endpoint=os.getenv(\"azure_tts_endpoint\")\n",
    "azure_tts_key=os.getenv(\"azure_tts_key\")\n",
    "azure_oai_endpoint=os.getenv(\"AZURE_OAI_ENDPOINT\")\n",
    "azure_oai_key=os.getenv(\"AZURE_OAI_KEY\")\n",
    "\n",
    "# STT(음성 → 텍스트) 요청 함수\n",
    "def request_stt(audio_path):\n",
    "    endpoints=azure_stt_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_stt_key\n",
    "        }    \n",
    "    with open(audio_path, 'rb') as audio_files:\n",
    "        audio_data=audio_files.read()\n",
    "    response = requests.post(endpoints,headers=headers,data=audio_data)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    response_json=response.json()\n",
    "    content=response_json['DisplayText']\n",
    "    return content\n",
    "\n",
    "# TTS(텍스트 → 음성) 요청 함수\n",
    "def request_tts(text):\n",
    "    endpoints=azure_tts_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_tts_key,\n",
    "        \"Content-Type\":\"application/ssml+xml\",\n",
    "        \"X-Microsoft-OutputFormat\":\"audio-16khz-128kbitrate-mono-mp3\"\n",
    "        }\n",
    "    # SSML 형식으로 요청 본문 생성\n",
    "    body = f\"\"\"\n",
    "        <speak version='1.0' xml:lang='ko-KR'>\n",
    "            <voice xml:lang='ko-KR' xml:gender='Female' name='ko-KR-SunHiNeural'>\n",
    "                {text}\n",
    "            </voice>\n",
    "        </speak>\n",
    "    \"\"\"\n",
    "    response = requests.post(endpoints, headers=headers, data=body)\n",
    "    if response.status_code !=200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None    \n",
    "    import datetime\n",
    "    now=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"tts_result{now}.wav\"\n",
    "    with open(filename,\"wb\") as audio_files:\n",
    "        audio_files.write(response.content)\n",
    "    return filename\n",
    "\n",
    "# def request_gpt(text):\n",
    "#     endpoint = azure_oai_endpoint\n",
    "#     headers={\n",
    "#         \"Authorization\":azure_oai_key\n",
    "#     }\n",
    "#     body={\n",
    "#         \"messages\":[\n",
    "#             {\"role\":\"user\",\n",
    "#              \"content\":text},\n",
    "#         ],\n",
    "#         \"max_tokens\":800,\n",
    "#         \"temperature\":0.7,\n",
    "#         \"top_p\":0.5,\n",
    "#         \"frequency_penalty\":0,\n",
    "#         \"model\":\"7ai053-gpt-4o-mini\"\n",
    "#     }\n",
    "#     response=requests.post(endpoint,headers=headers,json=body)\n",
    "\n",
    "#     if response.status_code != 200:\n",
    "#         return None\n",
    "#     response_json=response.json()\n",
    "#     content=response_json[\"choices\"][0][\"message\"]['content']\n",
    "#     return content\n",
    "\n",
    "# GPT API 요청 함수 -> 64~86번째 줄 환경변수 에러로 하드코딩\n",
    "def request_gpt(text):\n",
    "    endpoint = \"https://fimtrus-ai-project-resource.cognitiveservices.azure.com/openai/deployments/fimtrus-gpt-41/chat/completions?api-version=2025-01-01-preview\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer 9VEZmMXEZd4aWzFIEVBKCse1NEa3en2LrD51oEyXbFc41XDbcP2VJQQJ99BFACYeBjFXJ3w3AAAAACOGydeD\"\n",
    "    }\n",
    "    body = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }\n",
    "        ],\n",
    "        \"max_completion_tokens\": 800,\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 1,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"model\": \"fimtrus-gpt-41\"\n",
    "    }\n",
    "    response = requests.post(endpoint, headers=headers, json=body)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "    response_json = response.json()\n",
    "    content = response_json['choices'][0]['message']['content']\n",
    "    return content\n",
    " \n",
    " \n",
    "# Gradio UI 정의\n",
    "with gr.Blocks() as demo:\n",
    "    # 오디오 입력이 변경될 때 STT 실행\n",
    "    def change_audio(audio_path):\n",
    "        if audio_path is None:\n",
    "            return None\n",
    "        \n",
    "        content = request_stt(audio_path)\n",
    "        return content\n",
    "    \n",
    "    # TTS 버튼 클릭 시 실행\n",
    "    def click_send_tts(text):\n",
    "        filename=request_tts(text)\n",
    "        return filename\n",
    "    \n",
    "    # GPT 전송 버튼 클릭 시 실행  \n",
    "    def send_gpt(text,histories):\n",
    "        content = request_gpt(text)\n",
    "        # 히스토리를 만들어서, 챗봇에 넣어주면 된다.\n",
    "        print(histories)\n",
    "        print(content)\n",
    "        histories.append({\"role\":\"user\",\"content\":text})\n",
    "        histories.append({\"role\":\"assistant\",\"content\":content})\n",
    "        filename = request_tts(content)\n",
    "        return histories, filename\n",
    "    \n",
    "#Wrapper     # UI 레이아웃 구성\n",
    "    with gr.Row(): #가로 배치용\n",
    "        #좌측 (챗봇) , 입력창, GPT 음성 출력\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot=gr.Chatbot(type=\"messages\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=3):\n",
    "                    prompt_textbox = gr.Textbox(label=\"메세지 입력\",placeholder=\"여기에 질문을 입력하세요\")\n",
    "                with gr.Column(scale=1):\n",
    "                    gpt_send_button=gr.Button(\"전송\")\n",
    "            gpt_audio = gr.Audio(label=\"GPT 음성 출력\", type=\"filepath\",interactive=False,autoplay=True)\n",
    "            \n",
    "        #우측 (STT, TTS) \n",
    "        with gr.Column(scale=1):\n",
    "            #STT\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### STT\")\n",
    "                input_audio = gr.Audio(sources=\"microphone\", type=\"filepath\",label=\"마이크 입력\")\n",
    "                output_text = gr.Textbox(label=\"음성 인식 결과\", placeholder=\"여기에 음성 인식이 표시됩니다.\", interactive=False)\n",
    "\n",
    "                input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "            #TTS\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### TTS\")\n",
    "                tts_textbox = gr.Textbox(label=\"텍스트 입력\", placeholder=\"텍스트를 입력하세요\")\n",
    "                send_tts_button = gr.Button(\"음성으로 변환\")\n",
    "                output_tts_audio = gr.Audio(label=\"음성 출력\", type=\"filepath\", interactive=False,autoplay=True)\n",
    "\n",
    "    # 이벤트 연결\n",
    "    input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "    send_tts_button.click(click_send_tts, inputs=[tts_textbox],outputs=[output_tts_audio])\n",
    "    gpt_send_button.click(send_gpt,inputs=[prompt_textbox,chatbot],outputs=[chatbot, gpt_audio])\n",
    "\n",
    "# Gradio 앱 실행\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "957c46b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STT (Speech to Text)\n",
    "# TTS (Text to Speech) : 수정필요 (AS-IS: 리퀘스트 TTS까지 끝이 나야 음성에 대한 정보와 그리고 챗봇의 채팅이 업데이트/TO-BE :GPT로부터 컨텐츠를 받았을 때 바로 챗봇이 업데이트 )\n",
    "# Gradio Wrapper (좌:chatbot 우:STT, TTS)\n",
    "# Open AI (TTS / STT 독자 기능으로 연결)\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "from openai import AzureOpenAI \n",
    "import os\n",
    "import pdb\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "azure_stt_endpoint=os.getenv(\"azure_stt_endpoint\")\n",
    "azure_stt_key=os.getenv(\"azure_stt_key\")\n",
    "azure_tts_endpoint=os.getenv(\"azure_tts_endpoint\")\n",
    "azure_tts_key=os.getenv(\"azure_tts_key\")\n",
    "azure_oai_endpoint=os.getenv(\"AZURE_OAI_ENDPOINT\")\n",
    "azure_oai_key=os.getenv(\"AZURE_OAI_KEY\")\n",
    "\n",
    "# STT(음성 → 텍스트) 요청 함수\n",
    "def request_stt(audio_path):\n",
    "    endpoints=azure_stt_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_stt_key\n",
    "        }    \n",
    "    with open(audio_path, 'rb') as audio_files:\n",
    "        audio_data=audio_files.read()\n",
    "    response = requests.post(endpoints,headers=headers,data=audio_data)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    response_json=response.json()\n",
    "    content=response_json['DisplayText']\n",
    "    return content\n",
    "\n",
    "# TTS(텍스트 → 음성) 요청 함수\n",
    "def request_tts(text):\n",
    "    endpoints=azure_tts_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_tts_key,\n",
    "        \"Content-Type\":\"application/ssml+xml\",\n",
    "        \"X-Microsoft-OutputFormat\":\"audio-16khz-128kbitrate-mono-mp3\"\n",
    "        }\n",
    "    # SSML 형식으로 요청 본문 생성\n",
    "    body = f\"\"\"\n",
    "        <speak version='1.0' xml:lang='ko-KR'>\n",
    "            <voice xml:lang='ko-KR' xml:gender='Female' name='ko-KR-SunHiNeural'>\n",
    "                {text}\n",
    "            </voice>\n",
    "        </speak>\n",
    "    \"\"\"\n",
    "    response = requests.post(endpoints, headers=headers, data=body)\n",
    "    if response.status_code !=200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None    \n",
    "    import datetime\n",
    "    now=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"tts_result{now}.wav\"\n",
    "    with open(filename,\"wb\") as audio_files:\n",
    "        audio_files.write(response.content)\n",
    "    return filename\n",
    "\n",
    "# def request_gpt(text):\n",
    "#     endpoint = azure_oai_endpoint\n",
    "#     headers={\n",
    "#         \"Authorization\":azure_oai_key\n",
    "#     }\n",
    "#     body={\n",
    "#         \"messages\":[\n",
    "#             {\"role\":\"user\",\n",
    "#              \"content\":text},\n",
    "#         ],\n",
    "#         \"max_tokens\":800,\n",
    "#         \"temperature\":0.7,\n",
    "#         \"top_p\":0.5,\n",
    "#         \"frequency_penalty\":0,\n",
    "#         \"model\":\"7ai053-gpt-4o-mini\"\n",
    "#     }\n",
    "#     response=requests.post(endpoint,headers=headers,json=body)\n",
    "\n",
    "#     if response.status_code != 200:\n",
    "#         return None\n",
    "#     response_json=response.json()\n",
    "#     content=response_json[\"choices\"][0][\"message\"]['content']\n",
    "#     return content\n",
    "\n",
    "# GPT API 요청 함수 -> 64~86번째 줄 환경변수 에러로 하드코딩\n",
    "def request_gpt(text):\n",
    "    endpoint = \"https://fimtrus-ai-project-resource.cognitiveservices.azure.com/openai/deployments/fimtrus-gpt-41/chat/completions?api-version=2025-01-01-preview\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer 9VEZmMXEZd4aWzFIEVBKCse1NEa3en2LrD51oEyXbFc41XDbcP2VJQQJ99BFACYeBjFXJ3w3AAAAACOGydeD\"\n",
    "    }\n",
    "    body = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }\n",
    "        ],\n",
    "        \"max_completion_tokens\": 800,\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 1,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"model\": \"fimtrus-gpt-41\"\n",
    "    }\n",
    "    response = requests.post(endpoint, headers=headers, json=body)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "    response_json = response.json()\n",
    "    content = response_json['choices'][0]['message']['content']\n",
    "    return content\n",
    " \n",
    " \n",
    "# Gradio UI 정의\n",
    "with gr.Blocks() as demo:\n",
    "    # 오디오 입력이 변경될 때 STT 실행\n",
    "    def change_audio(audio_path):\n",
    "        if audio_path is None:\n",
    "            return None\n",
    "        content = request_stt(audio_path)\n",
    "        return content\n",
    "    \n",
    "    # TTS 버튼 클릭 시 실행\n",
    "    def click_send_tts(text):\n",
    "        filename=request_tts(text)\n",
    "        return filename\n",
    "    \n",
    "    # GPT 전송 버튼 클릭 시 실행  \n",
    "    def send_gpt(text,histories):\n",
    "        content = request_gpt(text) \n",
    "\n",
    "        if histories is None:\n",
    "            histories = []\n",
    "        # 히스토리를 만들어서, 챗봇에 넣어주면 된다.\n",
    "        print(histories)\n",
    "        print(content)\n",
    "        histories.append({\"role\":\"user\",\"content\":text})\n",
    "        histories.append({\"role\":\"assistant\",\"content\":content})\n",
    "        filename = request_tts(content)    # TODO: GPT 응답을 바로 음성으로 변환하는 이유는? (문제점이 있을까?)\n",
    "        return histories, filename      # TODO: chatbot에 text도 보여주고 gpt 음성도 재생하려고 이 두 값을 반환하는 걸까?\n",
    "\n",
    "    def change_prompt_audio(audio_path):\n",
    "        text = request_stt(audio_path)\n",
    "        return text\n",
    "\n",
    "#Wrapper     # UI 레이아웃 구성\n",
    "    with gr.Row(): #가로 배치용\n",
    "        #좌측 (챗봇) , 입력창, GPT 음성 출력\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot=gr.Chatbot(type=\"messages\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    prompt_audio = gr.Audio(sources=\"microphone\",label=\"질문\",type=\"filepath\")\n",
    "                with gr.Column(scale=3):\n",
    "                    prompt_textbox = gr.Textbox(label=\"메세지 입력\",placeholder=\"여기에 질문을 입력하세요\")\n",
    "                with gr.Column(scale=1):\n",
    "                    gpt_send_button=gr.Button(\"전송\")\n",
    "            gpt_audio = gr.Audio(label=\"GPT 음성 출력\", type=\"filepath\",interactive=False,autoplay=True)\n",
    "            \n",
    "        #우측 (STT, TTS) \n",
    "        with gr.Column(scale=1):\n",
    "            #STT\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### STT\")\n",
    "                input_audio = gr.Audio(sources=\"microphone\", type=\"filepath\",label=\"마이크 입력\")\n",
    "                output_text = gr.Textbox(label=\"음성 인식 결과\", placeholder=\"여기에 음성 인식이 표시됩니다.\", interactive=False)\n",
    "\n",
    "                input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "            #TTS\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### TTS\")\n",
    "                tts_textbox = gr.Textbox(label=\"텍스트 입력\", placeholder=\"텍스트를 입력하세요\")\n",
    "                send_tts_button = gr.Button(\"음성으로 변환\")\n",
    "                output_tts_audio = gr.Audio(label=\"음성 출력\", type=\"filepath\", interactive=False,autoplay=True)\n",
    "\n",
    "    # 이벤트 연결\n",
    "    input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "    send_tts_button.click(click_send_tts, inputs=[tts_textbox],outputs=[output_tts_audio])\n",
    "    gpt_send_button.click(send_gpt,inputs=[prompt_textbox,chatbot],outputs=[chatbot, gpt_audio])\n",
    "\n",
    "    prompt_audio.input(change_prompt_audio,inputs=[prompt_audio],outputs=[prompt_textbox])\n",
    "# Gradio 앱 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49c9c9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STT (Speech to Text)\n",
    "# TTS (Text to Speech) : 수정필요 (AS-IS: 리퀘스트 TTS까지 끝이 나야 음성에 대한 정보와 그리고 챗봇의 채팅이 업데이트/TO-BE :GPT로부터 컨텐츠를 받았을 때 바로 챗봇이 업데이트 )\n",
    "# Gradio Wrapper (좌:chatbot 우:STT, TTS)\n",
    "# Open AI (TTS / STT 동기 -> 비동기)\n",
    "    # 사용자 경험 개선 (GPT 응답 → 바로 챗봇 업데이트, TTS는 뒤늦게 처리되어도 괜찮음) 1\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "from openai import AzureOpenAI \n",
    "import os\n",
    "import pdb\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "azure_stt_endpoint=os.getenv(\"azure_stt_endpoint\")\n",
    "azure_stt_key=os.getenv(\"azure_stt_key\")\n",
    "azure_tts_endpoint=os.getenv(\"azure_tts_endpoint\")\n",
    "azure_tts_key=os.getenv(\"azure_tts_key\")\n",
    "azure_oai_endpoint=os.getenv(\"AZURE_OAI_ENDPOINT\")\n",
    "azure_oai_key=os.getenv(\"AZURE_OAI_KEY\")\n",
    "\n",
    "# STT(음성 → 텍스트) 요청 함수\n",
    "def request_stt(audio_path):\n",
    "    endpoints=azure_stt_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_stt_key\n",
    "        }    \n",
    "    with open(audio_path, 'rb') as audio_files:\n",
    "        audio_data=audio_files.read()\n",
    "    response = requests.post(endpoints,headers=headers,data=audio_data)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    response_json=response.json()\n",
    "    content=response_json['DisplayText']\n",
    "    return content\n",
    "\n",
    "# TTS(텍스트 → 음성) 요청 함수\n",
    "def request_tts(text):\n",
    "    endpoints=azure_tts_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_tts_key,\n",
    "        \"Content-Type\":\"application/ssml+xml\",\n",
    "        \"X-Microsoft-OutputFormat\":\"audio-16khz-128kbitrate-mono-mp3\"\n",
    "        }\n",
    "    # SSML 형식으로 요청 본문 생성\n",
    "    body = f\"\"\"\n",
    "        <speak version='1.0' xml:lang='ko-KR'>\n",
    "            <voice xml:lang='ko-KR' xml:gender='Female' name='ko-KR-SunHiNeural'>\n",
    "                {text}\n",
    "            </voice>\n",
    "        </speak>\n",
    "    \"\"\"\n",
    "    response = requests.post(endpoints, headers=headers, data=body)\n",
    "    if response.status_code !=200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None    \n",
    "    import datetime\n",
    "    now=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"tts_result{now}.wav\"\n",
    "    with open(filename,\"wb\") as audio_files:\n",
    "        audio_files.write(response.content)\n",
    "    return filename\n",
    "\n",
    "# GPT API 요청 함수\n",
    "def request_gpt(text):\n",
    "    endpoint = azure_oai_endpoint\n",
    "    headers={\n",
    "        \"Authorization\":azure_oai_key\n",
    "    }\n",
    "    body={\n",
    "        \"messages\":[\n",
    "            {\"role\":\"user\",\n",
    "             \"content\":text},\n",
    "        ],\n",
    "        \"max_tokens\":800,\n",
    "        \"temperature\":0.7,\n",
    "        \"top_p\":0.5,\n",
    "        \"frequency_penalty\":0,\n",
    "        \"model\":\"7ai053-gpt-4o-mini\"\n",
    "    }\n",
    "    response=requests.post(endpoint,headers=headers,json=body)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f'Error: {response.status_code}')\n",
    "        return None\n",
    "    response_json=response.json()\n",
    "    content=response_json[\"choices\"][0][\"message\"]['content']\n",
    "    return content\n",
    " \n",
    "# Gradio UI 정의\n",
    "with gr.Blocks() as demo:\n",
    "    # 오디오 입력이 변경될 때 STT 실행\n",
    "    def change_audio(audio_path):\n",
    "        if audio_path is None:\n",
    "            return None\n",
    "        content = request_stt(audio_path)\n",
    "        return content\n",
    "    \n",
    "    # TTS 버튼 클릭 시 실행\n",
    "    def click_send_tts(text):\n",
    "        filename=request_tts(text)\n",
    "        return filename\n",
    "    \n",
    "    # GPT 전송 버튼 클릭 시 실행  \n",
    "    def send_gpt(text,histories):\n",
    "        content = request_gpt(text)\n",
    "        # 히스토리를 만들어서, 챗봇에 넣어주면 된다.\n",
    "        print(histories)\n",
    "        print(content)\n",
    "        histories.append({\"role\":\"user\",\"content\":text})\n",
    "        histories.append({\"role\":\"assistant\",\"content\":content})\n",
    "        filename = request_tts(content) # ✅ [AS-IS 방식] GPT 응답을 받은 후 TTS까지 처리한 다음 chatbot 업데이트\n",
    "        return histories, filename\n",
    "\n",
    "    def change_prompt_audio(audio_path):\n",
    "        text = request_stt(audio_path)\n",
    "        return text  # ✅ [추가됨] 오디오 → 텍스트를 즉시 prompt_textbox에 반영하기 위한 함수\n",
    "    \n",
    "\n",
    "\n",
    "#Wrapper     # UI 레이아웃 구성\n",
    "    with gr.Row(): #가로 배치용\n",
    "        #좌측 (챗봇) , 입력창, GPT 음성 출력\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot=gr.Chatbot(type=\"messages\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    prompt_audio = gr.Audio(sources=\"microphone\",label=\"질문\",type=\"filepath\")\n",
    "                with gr.Column(scale=3):\n",
    "                    prompt_textbox = gr.Textbox(label=\"메세지 입력\",placeholder=\"여기에 질문을 입력하세요\")\n",
    "                with gr.Column(scale=1):\n",
    "                    gpt_send_button=gr.Button(\"전송\")\n",
    "            gpt_audio = gr.Audio(label=\"GPT 음성 출력\", type=\"filepath\",interactive=False,autoplay=True)\n",
    "            \n",
    "        #우측 (STT, TTS) \n",
    "        with gr.Column(scale=1):\n",
    "            #STT\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### STT\")\n",
    "                input_audio = gr.Audio(sources=\"microphone\", type=\"filepath\",label=\"마이크 입력\")\n",
    "                output_text = gr.Textbox(label=\"음성 인식 결과\", placeholder=\"여기에 음성 인식이 표시됩니다.\", interactive=False)\n",
    "\n",
    "                input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "            #TTS\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### TTS\")\n",
    "                tts_textbox = gr.Textbox(label=\"텍스트 입력\", placeholder=\"텍스트를 입력하세요\")\n",
    "                send_tts_button = gr.Button(\"음성으로 변환\")\n",
    "                output_tts_audio = gr.Audio(label=\"음성 출력\", type=\"filepath\", interactive=False,autoplay=True)\n",
    "\n",
    "    # 이벤트 연결\n",
    "    input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "    send_tts_button.click(click_send_tts, inputs=[tts_textbox],outputs=[output_tts_audio])\n",
    "    gpt_send_button.click(send_gpt,inputs=[prompt_textbox,chatbot],outputs=[chatbot, gpt_audio])     # ✅ [기존] 전송 버튼 클릭 시 GPT 요청 + TTS 생성 → chatbot, gpt_audio에 한 번에 업데이트\n",
    "\n",
    "    prompt_audio.input(change_prompt_audio,inputs=[prompt_audio],outputs=[prompt_textbox])     # ✅ [새로 추가] 마이크로 질문을 입력 → STT 결과를 텍스트박스에 자동 반영 (음성 텍스트 입력 지원)\n",
    "\n",
    "# Gradio 앱 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1e54f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change Prompt 네, 테스트 정상적으로 잘 작동합니다! 궁금한 점이나 요청사항 있으시면 말씀해 주세요. 😊\n"
     ]
    }
   ],
   "source": [
    "# STT (Speech to Text)\n",
    "# TTS (Text to Speech) : 수정필요 (AS-IS: 리퀘스트 TTS까지 끝이 나야 음성에 대한 정보와 그리고 챗봇의 채팅이 업데이트/TO-BE :GPT로부터 컨텐츠를 받았을 때 바로 챗봇이 업데이트 )\n",
    "# Gradio Wrapper (좌:chatbot 우:STT, TTS)\n",
    "# Open AI (TTS / STT 동기 -> 비동기)\n",
    "    # 사용자 경험 개선 (GPT 응답 → 바로 챗봇 업데이트, TTS는 뒤늦게 처리되어도 괜찮음) 2\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "from openai import AzureOpenAI \n",
    "import os\n",
    "import pdb\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "azure_stt_endpoint=os.getenv(\"azure_stt_endpoint\")\n",
    "azure_stt_key=os.getenv(\"azure_stt_key\")\n",
    "azure_tts_endpoint=os.getenv(\"azure_tts_endpoint\")\n",
    "azure_tts_key=os.getenv(\"azure_tts_key\")\n",
    "azure_oai_endpoint=os.getenv(\"AZURE_OAI_ENDPOINT\")\n",
    "azure_oai_key=os.getenv(\"AZURE_OAI_KEY\")\n",
    "\n",
    "# STT(음성 → 텍스트) 요청 함수\n",
    "def request_stt(audio_path):\n",
    "    endpoints=azure_stt_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_stt_key\n",
    "        }    \n",
    "    with open(audio_path, 'rb') as audio_files:\n",
    "        audio_data=audio_files.read()\n",
    "    response = requests.post(endpoints,headers=headers,data=audio_data)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    response_json=response.json()\n",
    "    content=response_json['DisplayText']\n",
    "    return content\n",
    "\n",
    "# TTS(텍스트 → 음성) 요청 함수\n",
    "def request_tts(text):\n",
    "    endpoints=azure_tts_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_tts_key,\n",
    "        \"Content-Type\":\"application/ssml+xml\",\n",
    "        \"X-Microsoft-OutputFormat\":\"audio-16khz-128kbitrate-mono-mp3\"\n",
    "        }\n",
    "    # SSML 형식으로 요청 본문 생성\n",
    "    body = f\"\"\"\n",
    "        <speak version='1.0' xml:lang='ko-KR'>\n",
    "            <voice xml:lang='ko-KR' xml:gender='Female' name='ko-KR-SunHiNeural'>\n",
    "                {text}\n",
    "            </voice>\n",
    "        </speak>\n",
    "    \"\"\"\n",
    "    response = requests.post(endpoints, headers=headers, data=body)\n",
    "    if response.status_code !=200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None    \n",
    "    import datetime\n",
    "    now=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"tts_result{now}.wav\"\n",
    "    with open(filename,\"wb\") as audio_files:\n",
    "        audio_files.write(response.content)\n",
    "    return filename\n",
    "\n",
    "def request_gpt(text):\n",
    "    endpoint = \"https://fimtrus-ai-project-resource.cognitiveservices.azure.com/openai/deployments/fimtrus-gpt-41/chat/completions?api-version=2025-01-01-preview\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer 9VEZmMXEZd4aWzFIEVBKCse1NEa3en2LrD51oEyXbFc41XDbcP2VJQQJ99BFACYeBjFXJ3w3AAAAACOGydeD\"\n",
    "    }\n",
    "    body = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }\n",
    "        ],\n",
    "        \"max_completion_tokens\": 800,\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 1,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"model\": \"fimtrus-gpt-41\"\n",
    "    }\n",
    "    response = requests.post(endpoint, headers=headers, json=body)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "    response_json = response.json()\n",
    "    content = response_json['choices'][0]['message']['content']\n",
    "    return content\n",
    " \n",
    " \n",
    "# Gradio UI 정의\n",
    "with gr.Blocks() as demo:\n",
    "    # 오디오 입력이 변경될 때 STT 실행\n",
    "    def change_audio(audio_path):\n",
    "        if audio_path is None:\n",
    "            return None\n",
    "        content = request_stt(audio_path)\n",
    "        return content\n",
    "    \n",
    "    # TTS 버튼 클릭 시 실행\n",
    "    def click_send_tts(text):\n",
    "        filename=request_tts(text)\n",
    "        return filename\n",
    "    \n",
    "    # GPT 전송 버튼 클릭 시 실행  \n",
    "    def send_gpt(text,histories):\n",
    "        content= request_gpt(text)\n",
    "        # 히스토리를 만들어서, 챗봇에 넣어주면 된다.\n",
    "        print(histories)\n",
    "        print(content)\n",
    "        histories.append({\"role\":\"user\",\"content\":text})\n",
    "        histories.append({\"role\":\"assistant\",\"content\":content})\n",
    "        filename = request_tts(content) \n",
    "        return histories, filename\n",
    "\n",
    "    def change_prompt_audio(audio_path):\n",
    "        text = request_stt(audio_path)\n",
    "        return text  \n",
    "    \n",
    "    \n",
    "    def change_prompt_text(text,histories):\n",
    "         content = request_gpt(text) # request_gpt가 있어서, 비동기적으로 처리하게 됨.\n",
    "         print(\"Change Prompt\", content)\n",
    "         histories.append({\"role\":\"user\",\"content\":text})\n",
    "         histories.append({\"role\":\"assistant\",\"content\":content})\n",
    "         return histories  # ✅ [추가됨] 텍스트 입력창이 바뀌면 즉시 GPT 응답 받고 챗봇 업데이트    \n",
    "\n",
    "#Wrapper     # UI 레이아웃 구성\n",
    "    with gr.Row(): #가로 배치용\n",
    "        #좌측 (챗봇) , 입력창, GPT 음성 출력\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot=gr.Chatbot(type=\"messages\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    prompt_audio = gr.Audio(sources=\"microphone\",label=\"질문\",type=\"filepath\")\n",
    "                with gr.Column(scale=3):\n",
    "                    prompt_textbox = gr.Textbox(label=\"메세지 입력\",placeholder=\"여기에 질문을 입력하세요\")\n",
    "                with gr.Column(scale=1):\n",
    "                    gpt_send_button=gr.Button(\"전송\")\n",
    "            gpt_audio = gr.Audio(label=\"GPT 음성 출력\", type=\"filepath\",interactive=False,autoplay=True)\n",
    "            \n",
    "        #우측 (STT, TTS) \n",
    "        with gr.Column(scale=1):\n",
    "            #STT\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### STT\")\n",
    "                input_audio = gr.Audio(sources=\"microphone\", type=\"filepath\",label=\"마이크 입력\")\n",
    "                output_text = gr.Textbox(label=\"음성 인식 결과\", placeholder=\"여기에 음성 인식이 표시됩니다.\", interactive=False)\n",
    "\n",
    "                input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "            #TTS\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### TTS\")\n",
    "                tts_textbox = gr.Textbox(label=\"텍스트 입력\", placeholder=\"텍스트를 입력하세요\")\n",
    "                send_tts_button = gr.Button(\"음성으로 변환\")\n",
    "                output_tts_audio = gr.Audio(label=\"음성 출력\", type=\"filepath\", interactive=False,autoplay=True)\n",
    "\n",
    "    # 이벤트 연결\n",
    "    input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "    # '전송'버튼 클리기 시, 챗봇 업데이트 및 GPT음성 출력\n",
    "    send_tts_button.click(click_send_tts, inputs=[tts_textbox],outputs=[output_tts_audio])\n",
    "    \n",
    "    gpt_send_button.click(send_gpt,inputs=[prompt_textbox,chatbot],outputs=[chatbot, gpt_audio])    \n",
    "    # 음성 입력 시, 텍스트 박스 업테이트\n",
    "    prompt_audio.input(change_prompt_audio,inputs=[prompt_audio],outputs=[prompt_textbox])\n",
    "    # 텍스트박스 내용 변경 시 챗봇 업테이트\n",
    "    prompt_textbox.change(change_prompt_text,inputs=[prompt_textbox,chatbot],outputs=[chatbot])     # ✅ [새로 추가] 텍스트 입력창에 내용이 바뀌면 자동으로 GPT 요청하고, 챗봇만 업데이트 (TTS는 없음)\n",
    "# Gradio 앱 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cca802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change Prompt 네! 무엇을 도와드릴까요?\n",
      "Change Prompt 배고프군요! 지금 뭐가 드시고 싶으세요? 간단하게 만들 수 있는 간식이나 요리를 추천해 드릴 수도 있는데, 어떤 걸 원하세요? 집에 있는 재료나 선호하는 음식 종류가 있으면 말씀해 주세요.\n"
     ]
    }
   ],
   "source": [
    "# STT (Speech to Text)\n",
    "# TTS (Text to Speech) : 수정필요 (AS-IS: 리퀘스트 TTS까지 끝이 나야 음성에 대한 정보와 그리고 챗봇의 채팅이 업데이트/TO-BE :GPT로부터 컨텐츠를 받았을 때 바로 챗봇이 업데이트 )\n",
    "# Gradio Wrapper (좌:chatbot 우:STT, TTS)\n",
    "# Open AI (TTS / STT 동기 -> 비동기)\n",
    "    # 사용자 경험 개선 (GPT 응답 → 바로 챗봇 업데이트, TTS는 뒤늦게 처리되어도 괜찮음)\n",
    "    # 사용자 경험 개선 (Gradio 앱에서 음성 → 텍스트 → GPT 응답 → 음성 변환까지의 전체 흐름을 비동기적 방식으로 처리하는 구조)\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "from openai import AzureOpenAI \n",
    "import os\n",
    "import pdb\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "azure_stt_endpoint=os.getenv(\"azure_stt_endpoint\")\n",
    "azure_stt_key=os.getenv(\"azure_stt_key\")\n",
    "azure_tts_endpoint=os.getenv(\"azure_tts_endpoint\")\n",
    "azure_tts_key=os.getenv(\"azure_tts_key\")\n",
    "azure_oai_endpoint=os.getenv(\"AZURE_OAI_ENDPOINT\")\n",
    "azure_oai_key=os.getenv(\"AZURE_OAI_KEY\")\n",
    "\n",
    "# STT(음성 → 텍스트) 요청 함수\n",
    "def request_stt(audio_path):\n",
    "    endpoints=azure_stt_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_stt_key\n",
    "        }    \n",
    "    with open(audio_path, 'rb') as audio_files:\n",
    "        audio_data=audio_files.read()\n",
    "    response = requests.post(endpoints,headers=headers,data=audio_data)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    response_json=response.json()\n",
    "    content=response_json['DisplayText']\n",
    "    return content\n",
    "\n",
    "# TTS(텍스트 → 음성) 요청 함수\n",
    "def request_tts(text):\n",
    "    endpoints=azure_tts_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_tts_key,\n",
    "        \"Content-Type\":\"application/ssml+xml\",\n",
    "        \"X-Microsoft-OutputFormat\":\"audio-16khz-128kbitrate-mono-mp3\"\n",
    "        }\n",
    "    # SSML 형식으로 요청 본문 생성\n",
    "    body = f\"\"\"\n",
    "        <speak version='1.0' xml:lang='ko-KR'>\n",
    "            <voice xml:lang='ko-KR' xml:gender='Female' name='ko-KR-SunHiNeural'>\n",
    "                {text}\n",
    "            </voice>\n",
    "        </speak>\n",
    "    \"\"\"\n",
    "    response = requests.post(endpoints, headers=headers, data=body)\n",
    "    if response.status_code !=200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None    \n",
    "    import datetime\n",
    "    now=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"tts_result{now}.wav\"\n",
    "    with open(filename,\"wb\") as audio_files:\n",
    "        audio_files.write(response.content)\n",
    "    return filename\n",
    "\n",
    "# GPT API 요청 함수 -> 70~92번째 줄 환경변수 에러로 하드코딩\n",
    "def request_gpt(text):\n",
    "    endpoint = \"https://fimtrus-ai-project-resource.cognitiveservices.azure.com/openai/deployments/fimtrus-gpt-41/chat/completions?api-version=2025-01-01-preview\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer 9VEZmMXEZd4aWzFIEVBKCse1NEa3en2LrD51oEyXbFc41XDbcP2VJQQJ99BFACYeBjFXJ3w3AAAAACOGydeD\"\n",
    "    }\n",
    "    body = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }\n",
    "        ],\n",
    "        \"max_completion_tokens\": 800,\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 1,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"model\": \"fimtrus-gpt-41\"\n",
    "    }\n",
    "    response = requests.post(endpoint, headers=headers, json=body)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "    response_json = response.json()\n",
    "    content = response_json['choices'][0]['message']['content']\n",
    "    return content\n",
    " \n",
    " \n",
    "# Gradio UI 정의\n",
    "with gr.Blocks() as demo:\n",
    "    # 오디오 입력이 변경될 때 STT 실행\n",
    "    def change_audio(audio_path):\n",
    "        if audio_path is None:\n",
    "            return None\n",
    "        content = request_stt(audio_path)\n",
    "        return content\n",
    "    \n",
    "    # TTS 버튼 클릭 시 실행\n",
    "    def click_send_tts(text):\n",
    "        filename=request_tts(text)\n",
    "        return filename\n",
    "    \n",
    "    # GPT 전송 버튼 클릭 시 실행  \n",
    "    def send_gpt(text,histories):\n",
    "        content = request_gpt(text)\n",
    "        print(histories)\n",
    "        print(content)\n",
    "        # 대화 기록에 사용자/AI 메시지 추가\n",
    "        histories.append({\"role\":\"user\",\"content\":text})\n",
    "        histories.append({\"role\":\"assistant\",\"content\":content})\n",
    "        # 챗봇 이벤트로 이동해야함.\n",
    "        filename = request_tts(content) \n",
    "        return histories, filename\n",
    "\n",
    "    def change_prompt_audio(audio_path):        \n",
    "        if audio_path is None:\n",
    "            return None\n",
    "        \n",
    "        text = request_stt(audio_path)\n",
    "        return text\n",
    "\n",
    "    def change_prompt_text(text,histories):\n",
    "        if text is None or text == \"\":\n",
    "            return histories\n",
    "        \n",
    "        content = request_gpt(text)\n",
    "        # 대화 기록에 사용자/AI 메시지 추가\n",
    "        histories.append({\"role\":\"user\",\"content\":text})\n",
    "        histories.append({\"role\":\"assistant\",\"content\":content})\n",
    "        return histories\n",
    "\n",
    "#Wrapper     # UI 레이아웃 구성\n",
    "    with gr.Row(): #가로 배치용\n",
    "        #좌측 (챗봇) , 입력창, GPT 음성 출력\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot=gr.Chatbot(type=\"messages\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    prompt_audio = gr.Audio(sources=\"microphone\",label=\"질문\",type=\"filepath\")\n",
    "                with gr.Column(scale=3):\n",
    "                    prompt_textbox = gr.Textbox(label=\"메세지 입력\",placeholder=\"여기에 질문을 입력하세요\")\n",
    "                with gr.Column(scale=1):\n",
    "                    gpt_send_button=gr.Button(\"전송\")\n",
    "            gpt_audio = gr.Audio(label=\"GPT 음성 출력\", type=\"filepath\",interactive=False,autoplay=True)\n",
    "            \n",
    "        #우측 (STT, TTS) \n",
    "        with gr.Column(scale=1):\n",
    "            #STT\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### STT\")\n",
    "                input_audio = gr.Audio(sources=\"microphone\", type=\"filepath\",label=\"마이크 입력\")\n",
    "                output_text = gr.Textbox(label=\"음성 인식 결과\", placeholder=\"여기에 음성 인식이 표시됩니다.\", interactive=False)\n",
    "\n",
    "                input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "            #TTS\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### TTS\")\n",
    "                tts_textbox = gr.Textbox(label=\"텍스트 입력\", placeholder=\"텍스트를 입력하세요\")\n",
    "                send_tts_button = gr.Button(\"음성으로 변환\")\n",
    "                output_tts_audio = gr.Audio(label=\"음성 출력\", type=\"filepath\", interactive=False,autoplay=True)\n",
    "\n",
    "    # 이벤트 연결\n",
    "    input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "    send_tts_button.click(click_send_tts, inputs=[tts_textbox],outputs=[output_tts_audio])\n",
    "    gpt_send_button.click(send_gpt,inputs=[prompt_textbox,chatbot],outputs=[chatbot, gpt_audio])\n",
    "\n",
    "    prompt_audio.input(change_prompt_audio,inputs=[prompt_audio],outputs=[prompt_textbox])     \n",
    "    prompt_textbox.change(change_prompt_text,inputs=[prompt_textbox,chatbot],outputs=[chatbot])     \n",
    "\n",
    "# Gradio 앱 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a83239d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'metadata': None, 'content': '안녕.', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': '안녕하세요! 무엇을 도와드릴까요? 😊', 'options': None}]\n",
      "[{'role': 'user', 'metadata': None, 'content': '안녕.', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': '안녕하세요! 무엇을 도와드릴까요? 😊', 'options': None}, {'role': 'user', 'metadata': None, 'content': '무엇을 도와줄 수 있는데?', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': '안녕하세요! 저는 다양한 주제에 대해 도움을 드릴 수 있어요. 예를 들면:\\n\\n- 궁금한 지식이나 정보 찾기\\n- 글쓰기(이메일, 자기소개서, 논문 등) 도와주기\\n- 언어 번역(한영, 영한 등)\\n- 학습 자료 요약 및 정리\\n- 코딩 및 프로그래밍 관련 질문 답변\\n- 여행, 요리, 취미 등의 추천\\n- 고민 상담이나 자기계발 팁 제공\\n\\n혹시 궁금한 점이나 도움이 필요한 일이 있으신가요? 구체적으로 말씀해주시면 더 잘 도와드릴 수 있습니다!', 'options': None}]\n"
     ]
    }
   ],
   "source": [
    "# STT (Speech to Text)\n",
    "# TTS (Text to Speech) : 수정필요 (AS-IS: 리퀘스트 TTS까지 끝이 나야 음성에 대한 정보와 그리고 챗봇의 채팅이 업데이트/TO-BE :GPT로부터 컨텐츠를 받았을 때 바로 챗봇이 업데이트 )\n",
    "# Gradio Wrapper (좌:chatbot 우:STT, TTS)\n",
    "# Open AI (TTS / STT 동기 -> 비동기)\n",
    "    # 사용자 경험 개선 (GPT 응답 → 바로 챗봇 업데이트, TTS는 뒤늦게 처리되어도 괜찮음)\n",
    "    # 사용자 경험 개선 (Gradio 앱에서 음성 → 텍스트 → GPT 응답 → 여기까지//\n",
    "   \n",
    "import gradio as gr\n",
    "import requests\n",
    "from openai import AzureOpenAI \n",
    "import os\n",
    "import pdb\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "azure_stt_endpoint=os.getenv(\"azure_stt_endpoint\")\n",
    "azure_stt_key=os.getenv(\"azure_stt_key\")\n",
    "azure_tts_endpoint=os.getenv(\"azure_tts_endpoint\")\n",
    "azure_tts_key=os.getenv(\"azure_tts_key\")\n",
    "azure_oai_endpoint=os.getenv(\"AZURE_OAI_ENDPOINT\")\n",
    "azure_oai_key=os.getenv(\"AZURE_OAI_KEY\")\n",
    "\n",
    "# STT(음성 → 텍스트) 요청 함수\n",
    "def request_stt(audio_path):\n",
    "    endpoints=azure_stt_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_stt_key\n",
    "        }    \n",
    "    with open(audio_path, 'rb') as audio_files:\n",
    "        audio_data=audio_files.read()\n",
    "    response = requests.post(endpoints,headers=headers,data=audio_data)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    response_json=response.json()\n",
    "    content=response_json['DisplayText']\n",
    "    return content\n",
    "\n",
    "# TTS(텍스트 → 음성) 요청 함수\n",
    "def request_tts(text):\n",
    "    endpoints=azure_tts_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_tts_key,\n",
    "        \"Content-Type\":\"application/ssml+xml\",\n",
    "        \"X-Microsoft-OutputFormat\":\"audio-16khz-128kbitrate-mono-mp3\"\n",
    "        }\n",
    "    # SSML 형식으로 요청 본문 생성\n",
    "    body = f\"\"\"\n",
    "        <speak version='1.0' xml:lang='ko-KR'>\n",
    "            <voice xml:lang='ko-KR' xml:gender='Female' name='ko-KR-SunHiNeural'>\n",
    "                {text}\n",
    "            </voice>\n",
    "        </speak>\n",
    "    \"\"\"\n",
    "    response = requests.post(endpoints, headers=headers, data=body)\n",
    "    if response.status_code !=200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None    \n",
    "    import datetime\n",
    "    now=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"tts_result{now}.wav\"\n",
    "    with open(filename,\"wb\") as audio_files:\n",
    "        audio_files.write(response.content)\n",
    "    return filename\n",
    "\n",
    "# GPT API 요청 함수 -> 70~92번째 줄 환경변수 에러로 하드코딩\n",
    "def request_gpt(text):\n",
    "    endpoint = \"https://fimtrus-ai-project-resource.cognitiveservices.azure.com/openai/deployments/fimtrus-gpt-41/chat/completions?api-version=2025-01-01-preview\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer 9VEZmMXEZd4aWzFIEVBKCse1NEa3en2LrD51oEyXbFc41XDbcP2VJQQJ99BFACYeBjFXJ3w3AAAAACOGydeD\"\n",
    "    }\n",
    "    body = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }\n",
    "        ],\n",
    "        \"max_completion_tokens\": 800,\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 1,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"model\": \"fimtrus-gpt-41\"\n",
    "    }\n",
    "    response = requests.post(endpoint, headers=headers, json=body)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "    response_json = response.json()\n",
    "    content = response_json['choices'][0]['message']['content']\n",
    "    return content\n",
    " \n",
    " \n",
    "# Gradio UI 정의\n",
    "with gr.Blocks() as demo:\n",
    "    # 오디오 입력이 변경될 때 STT 실행\n",
    "    def change_audio(audio_path):\n",
    "        if audio_path is None:\n",
    "            return None\n",
    "        content = request_stt(audio_path)\n",
    "        return content\n",
    "    \n",
    "    # TTS 버튼 클릭 시 실행\n",
    "    def click_send_tts(text):\n",
    "        filename=request_tts(text)\n",
    "        return filename\n",
    "    \n",
    "    # GPT 전송 버튼 클릭 시 실행  \n",
    "    def send_gpt(text,histories):\n",
    "        content = request_gpt(text)\n",
    "        print(histories)\n",
    "        print(content)\n",
    "        # 대화 기록에 사용자/AI 메시지 추가\n",
    "        histories.append({\"role\":\"user\",\"content\":text})\n",
    "        histories.append({\"role\":\"assistant\",\"content\":content})\n",
    "        # 챗봇 이벤트로 이동해야함.\n",
    "        filename = request_tts(content) \n",
    "        return histories, filename\n",
    "\n",
    "    def change_prompt_audio(audio_path):        \n",
    "        if audio_path is None:\n",
    "            return None\n",
    "        \n",
    "        text = request_stt(audio_path)\n",
    "        return text\n",
    "\n",
    "    def change_prompt_text(text,histories):\n",
    "        if text is None or text == \"\":\n",
    "            return histories\n",
    "        \n",
    "        content = request_gpt(text)\n",
    "        # 대화 기록에 사용자/AI 메시지 추가\n",
    "        histories.append({\"role\":\"user\",\"content\":text})\n",
    "        histories.append({\"role\":\"assistant\",\"content\":content})\n",
    "        return histories\n",
    "    \n",
    "    # 파일에 데이터를 저장 할때 예쁘게 보기 좋게(들여쓰기 포함해서) 저장하는 방법\n",
    "    def change_chatbot(histories):\n",
    "        print(histories)\n",
    "\n",
    "#Wrapper     # UI 레이아웃 구성\n",
    "    with gr.Row(): #가로 배치용\n",
    "        #좌측 (챗봇) , 입력창, GPT 음성 출력\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot=gr.Chatbot(type=\"messages\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    prompt_audio = gr.Audio(sources=\"microphone\",label=\"질문\",type=\"filepath\")\n",
    "                with gr.Column(scale=3):\n",
    "                    prompt_textbox = gr.Textbox(label=\"메세지 입력\",placeholder=\"여기에 질문을 입력하세요\")\n",
    "                with gr.Column(scale=1):\n",
    "                    gpt_send_button=gr.Button(\"전송\")\n",
    "            gpt_audio = gr.Audio(label=\"GPT 음성 출력\", type=\"filepath\",interactive=False,autoplay=True)\n",
    "            \n",
    "        #우측 (STT, TTS) \n",
    "        with gr.Column(scale=1):\n",
    "            #STT\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### STT\")\n",
    "                input_audio = gr.Audio(sources=\"microphone\", type=\"filepath\",label=\"마이크 입력\")\n",
    "                output_text = gr.Textbox(label=\"음성 인식 결과\", placeholder=\"여기에 음성 인식이 표시됩니다.\", interactive=False)\n",
    "\n",
    "                input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "            #TTS\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### TTS\")\n",
    "                tts_textbox = gr.Textbox(label=\"텍스트 입력\", placeholder=\"텍스트를 입력하세요\")\n",
    "                send_tts_button = gr.Button(\"음성으로 변환\")\n",
    "                output_tts_audio = gr.Audio(label=\"음성 출력\", type=\"filepath\", interactive=False,autoplay=True)\n",
    "\n",
    "    # 이벤트 연결\n",
    "    input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "    send_tts_button.click(click_send_tts, inputs=[tts_textbox],outputs=[output_tts_audio])\n",
    "    gpt_send_button.click(send_gpt,inputs=[prompt_textbox,chatbot],outputs=[chatbot, gpt_audio])\n",
    "\n",
    "    prompt_audio.input(change_prompt_audio,inputs=[prompt_audio],outputs=[prompt_textbox])     \n",
    "    prompt_textbox.change(change_prompt_text,inputs=[prompt_textbox,chatbot],outputs=[chatbot])     \n",
    "    chatbot.change(change_chatbot,inputs=[chatbot],outputs=[])\n",
    "\n",
    "# Gradio 앱 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da89c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STT (Speech to Text)\n",
    "# TTS (Text to Speech) : 수정필요 (AS-IS: 리퀘스트 TTS까지 끝이 나야 음성에 대한 정보와 그리고 챗봇의 채팅이 업데이트/TO-BE :GPT로부터 컨텐츠를 받았을 때 바로 챗봇이 업데이트 )\n",
    "# Gradio Wrapper (좌:chatbot 우:STT, TTS)\n",
    "# Open AI (TTS / STT 동기 -> 비동기)\n",
    "    # 사용자 경험 개선 (GPT 응답 → 바로 챗봇 업데이트, TTS는 뒤늦게 처리되어도 괜찮음)\n",
    "    # 사용자 경험 개선 (Gradio 앱에서 음성 → 텍스트 → GPT 응답 → (json형태로 받아냈음)\n",
    "    # //여기서 부터 음성 변환까지의 전체 흐름을 비동기적 방식으로 처리하는 구조) : 순차적으로 트리거\n",
    "    #  \"사용자 정의 음성 선택 + 정규식 필터링\"\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "from openai import AzureOpenAI \n",
    "import os\n",
    "import pdb\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "azure_stt_endpoint=os.getenv(\"azure_stt_endpoint\")\n",
    "azure_stt_key=os.getenv(\"azure_stt_key\")\n",
    "azure_tts_endpoint=os.getenv(\"azure_tts_endpoint\")\n",
    "azure_tts_key=os.getenv(\"azure_tts_key\")\n",
    "azure_oai_endpoint=os.getenv(\"AZURE_OAI_ENDPOINT\")\n",
    "azure_oai_key=os.getenv(\"AZURE_OAI_KEY\")\n",
    "\n",
    "# STT(음성 → 텍스트) 요청 함수\n",
    "def request_stt(audio_path):\n",
    "    endpoints=azure_stt_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_stt_key\n",
    "        }    \n",
    "    with open(audio_path, 'rb') as audio_files:\n",
    "        audio_data=audio_files.read()\n",
    "    response = requests.post(endpoints,headers=headers,data=audio_data)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    response_json=response.json()\n",
    "    content=response_json['DisplayText']\n",
    "    return content\n",
    "\n",
    "# TTS(텍스트 → 음성) 요청 함수\n",
    "def request_tts(text, voice=\"ko-KR-SunHiNeural\"):\n",
    "    endpoints=azure_tts_endpoint\n",
    "    headers={\n",
    "        \"Ocp-Apim-Subscription-Key\":azure_tts_key,\n",
    "        \"Content-Type\":\"application/ssml+xml\",\n",
    "        \"X-Microsoft-OutputFormat\":\"audio-16khz-128kbitrate-mono-mp3\"\n",
    "        }\n",
    "    # SSML 형식으로 요청 본문 생성\n",
    "    body = f\"\"\"\n",
    "        <speak version='1.0' xml:lang='ko-KR'>\n",
    "            <voice xml:lang='ko-KR' xml:gender='Female' name='{voice}'>\n",
    "                {text}\n",
    "            </voice>\n",
    "        </speak>\n",
    "    \"\"\"\n",
    "    response = requests.post(endpoints, headers=headers, data=body)\n",
    "    if response.status_code !=200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None    \n",
    "    import datetime\n",
    "    now=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    filename = f\"tts_result{now}.wav\"\n",
    "    with open(filename,\"wb\") as audio_files:\n",
    "        audio_files.write(response.content)\n",
    "    return filename\n",
    "def request_gpt(text):\n",
    "    endpoint = \"https://fimtrus-ai-project-resource.cognitiveservices.azure.com/openai/deployments/fimtrus-gpt-41/chat/completions?api-version=2025-01-01-preview\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer 9VEZmMXEZd4aWzFIEVBKCse1NEa3en2LrD51oEyXbFc41XDbcP2VJQQJ99BFACYeBjFXJ3w3AAAAACOGydeD\"\n",
    "    }\n",
    "    body = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }\n",
    "        ],\n",
    "        \"max_completion_tokens\": 800,\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 1,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"model\": \"fimtrus-gpt-41\"\n",
    "    }\n",
    "    response = requests.post(endpoint, headers=headers, json=body)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "    response_json = response.json()\n",
    "    content = response_json['choices'][0]['message']['content']\n",
    "    return content\n",
    " \n",
    "# Gradio UI 정의\n",
    "with gr.Blocks() as demo:\n",
    "    # voice drop down\n",
    "    voice_list = [\n",
    "        \"ko-KR-SunHiNeural\",\n",
    "        \"ko-KR-InJoonNeural\",\n",
    "        \"ko-KR-HyunsuMultilingualNeural\",\n",
    "        \"ko-KR-BongJinNeural\",\n",
    "        \"ko-KR-GookMinNeural\",\n",
    "        \"ko-KR-HyunsuNeural\",\n",
    "        \"ko-KR-JiMinNeural\",\n",
    "        \"ko-KR-SeoHyeonNeural\",\n",
    "        \"ko-KR-SoonBokNeural\",\n",
    "        \"ko-KR-YuJinNeural\"\n",
    "        ]\n",
    "\n",
    "    # 오디오 입력이 변경될 때 STT 실행\n",
    "    def change_audio(audio_path):\n",
    "        if audio_path is None:\n",
    "            return None\n",
    "        content = request_stt(audio_path)\n",
    "        return content\n",
    "    \n",
    "    # TTS 버튼 클릭 시 실행\n",
    "    def click_send_tts(text):\n",
    "        filename=request_tts(text)\n",
    "        return filename\n",
    "    \n",
    "    # GPT 전송 버튼 클릭 시 실행  \n",
    "    def send_gpt(text,histories):\n",
    "        content = request_gpt(text)\n",
    "        print(histories)\n",
    "        print(content)\n",
    "        histories.append({\"role\":\"user\",\"content\":text})\n",
    "        histories.append({\"role\":\"assistant\",\"content\":content})\n",
    "        filename = request_tts(content) \n",
    "        return histories, filename\n",
    "\n",
    "    def change_prompt_audio(audio_path):\n",
    "\n",
    "        if audio_path is None:  # ✅ [변경됨] \n",
    "            return None\n",
    "        \n",
    "        text = request_stt(audio_path)\n",
    "        return text \n",
    "    \n",
    "    def change_prompt_text(text,histories):\n",
    "\n",
    "        if text is None or text == \"\":  # ✅ [변경됨] \n",
    "            return histories\n",
    "        \n",
    "        content = request_gpt(text) # ✅ [변경됨] \n",
    "\n",
    "        histories.append({\"role\":\"user\",\"content\":text})\n",
    "        histories.append({\"role\":\"assistant\",\"content\":content})\n",
    "\n",
    "        return histories\n",
    "    \n",
    "    def change_chatbot(histories, voice):\n",
    "        content = histories[-1]['content']\n",
    "        pattern = r'[^가-힣a-zA-Z0-9\\s!.,]'\n",
    "        cleaned_content = re.sub(pattern, '', content)\n",
    "        filename = request_tts(cleaned_content, voice)\n",
    "        \n",
    "        return filename\n",
    "    \n",
    "    def change_voice(voice):\n",
    "        return voice\n",
    "  \n",
    "\n",
    "# UI 레이아웃 구성\n",
    "    with gr.Row(): #가로 배치용\n",
    "        #좌측 (챗봇) , 입력창, GPT 음성 출력\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot=gr.Chatbot(type=\"messages\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    prompt_audio = gr.Audio(sources=\"microphone\",label=\"질문\",type=\"filepath\")\n",
    "\n",
    "                with gr.Column(scale=3):\n",
    "                    prompt_textbox = gr.Textbox(label=\"메세지 입력\",placeholder=\"여기에 메시지를 입력하세요\")\n",
    "\n",
    "                                        \n",
    "                # with gr.Column(scale=1):\n",
    "                #     send_gpt_button = gr.Button(\"전송\")\n",
    "            voice_dropdown = gr.Dropdown(choices=voice_list, label=\"음성 선택\", value=\"ko-KR-SunHiNeural\", type=\"value\")  # ✅ [변경됨] \n",
    "\n",
    "            gpt_audio=gr.Audio(label=\"GPT 음성 출력\", type=\"filepath\",interactive=False,autoplay=True)          # ✅ [변경됨] \n",
    "\n",
    "            \n",
    "        #우측 (STT, TTS) \n",
    "        with gr.Column(scale=1):\n",
    "            #STT\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### STT\")\n",
    "                input_audio = gr.Audio(sources=\"microphone\", type=\"filepath\",label=\"마이크 입력\")\n",
    "                output_text = gr.Textbox(label=\"음성 인식 결과\", placeholder=\"여기에 음성 인식이 표시됩니다.\", interactive=False)\n",
    "\n",
    "                #input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])   # ✅ [변경됨] \n",
    "            #TTS\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### TTS\")\n",
    "                tts_textbox = gr.Textbox(label=\"텍스트 입력\", placeholder=\"텍스트를 입력하세요\")\n",
    "                send_tts_button = gr.Button(\"음성으로 변환\")\n",
    "                output_tts_audio = gr.Audio(label=\"음성 출력\", type=\"filepath\", interactive=False,autoplay=True)\n",
    "                \n",
    "\n",
    "    # 이벤트 연결\n",
    "    input_audio.change(change_audio, inputs=[input_audio],outputs=[output_text])\n",
    "    send_tts_button.click(click_send_tts, inputs=[tts_textbox],outputs=[output_tts_audio])\n",
    "    # gpt_send_button.click(send_gpt,inputs=[prompt_textbox,chatbot],outputs=[chatbot, gpt_audio])      # ✅ [변경됨] \n",
    "\n",
    "    prompt_audio.input(change_prompt_audio,inputs=[prompt_audio],outputs=[prompt_textbox])     \n",
    "    prompt_textbox.change(change_prompt_text,inputs=[prompt_textbox,chatbot],outputs=[chatbot])    # ✅ [변경됨] \n",
    "\n",
    "    chatbot.change(change_chatbot, inputs=[chatbot, voice_dropdown], outputs=[gpt_audio])      # ✅ [변경됨] \n",
    "    voice_dropdown.change(change_voice, inputs=[voice_dropdown], outputs=[voice_dropdown])      # ✅ [변경됨] \n",
    "\n",
    "\n",
    "# Gradio 앱 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802c923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
